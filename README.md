## Real-Time Stock Market Data Analysis using Kafka and AWS

### Introduction

In this project, we will be embarking on an end-to-end data engineering journey to analyze real-time stock market data. To achieve this, we will leverage various cutting-edge technologies, including Python, Amazon Web Services (AWS), Apache Kafka, Glue, Athena, and SQL.

### Architecture
![Architecture](https://github.com/piyushmishr/Stock-Market-Real-Time-Data-Analysis-/assets/81643238/60fced19-7da8-4bc1-a36f-4492d175e096)


### Technology Used

Our project revolves around the integration of various technologies to achieve our data engineering goals:

- **Programming Language - Python:** Python will be our go-to language for writing data processing scripts, interacting with AWS services, and performing necessary transformations.

- **Amazon Web Service (AWS):** We will harness the power of AWS to deploy our architecture in the cloud, making use of its storage, computing, and analytics services.

- **S3 (Simple Storage Service):** S3 will store both our raw and processed stock market data, providing us with a scalable and durable storage solution.

- **Athena:** Athena will empower us to analyze our data using SQL queries, making the data analysis process efficient and accessible.

- **Glue Crawler:** With Glue Crawler, we can automate the discovery and cataloging of data, ensuring that our data catalog remains up to date.

- **Glue Catalog:** The Glue Catalog acts as a centralized metadata repository, enhancing data organization and accessibility.

- **EC2 (Elastic Compute Cloud):** EC2 instances will be utilized for computationally intensive tasks and specific software requirements.

- **Apache Kafka:** Kafka will enable us to process real-time stock market data streams efficiently, opening doors to various real-time analytics possibilities.

Through this project, we will gain hands-on experience in building a comprehensive data engineering solution that handles real-time stock market data efficiently and effectively. By the end of this project, we'll have a deeper understanding of these technologies and how they can be orchestrated to create a robust data processing pipeline.
